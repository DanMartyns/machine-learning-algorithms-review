<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.1">Jekyll</generator><link href="http://localhost:4000/machine-learning-algorithms-review/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/machine-learning-algorithms-review/" rel="alternate" type="text/html" /><updated>2021-05-14T00:05:28+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/feed.xml</id><title type="html">ML Algorithms Review</title><subtitle>Construction of a web page with a review of some Machine Learning algorithms.</subtitle><author><name>Daniel Martins</name></author><entry><title type="html">Decision Trees</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Decision-Trees.html" rel="alternate" type="text/html" title="Decision Trees" /><published>2021-05-11T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Decision-Trees</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Decision-Trees.html">&lt;p&gt;Decision trees are widely used models for classification and regression tasks. Essentially, they predict a target by learning decision rules from features. As the name suggests, we can think of this model as breaking down our data by making a decision based on asking a series of questions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/011.jpeg&quot; alt=&quot;pair_plot&quot; width=&quot;70%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Based on the features in our training set, the decision tree model learns a series of questions to infer the class labels of the samples. As we can see, decision trees are attractive models if we care about interpretability.&lt;/p&gt;

&lt;p&gt;A decision tree is constructed by recursive partioning - starting from a root node (called first &lt;strong&gt;parent&lt;/strong&gt;), and each node can be split into left and right &lt;strong&gt;child&lt;/strong&gt; nodes. Nodes that do not have any child are known as &lt;strong&gt;terminal/leaf nodes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A decision tree makes decisions by splitting nodes into sub-nodes. This process is performed multiple times during the training process until only homogenous nodes are left. &lt;strong&gt;Node splitting&lt;/strong&gt;, or &lt;strong&gt;splitting&lt;/strong&gt;, is the process of dividing a node into multiple sub-nodes to create relatively terminal nodes. There are multiple ways of doing this, which can be broadly divided into two categories based on the type of target variable:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Continuous target variable
    &lt;ul&gt;
      &lt;li&gt;Reduction in Variance&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Categorical target variable
    &lt;ul&gt;
      &lt;li&gt;Gini Impurity&lt;/li&gt;
      &lt;li&gt;Entropy/Information Gain&lt;/li&gt;
      &lt;li&gt;Chi-Square&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Although the preceding figure illustrates the concept of a decision tree based on categorical targets (&lt;strong&gt;classification&lt;/strong&gt;), the same concept applies if our targets are real numbers (&lt;strong&gt;regression&lt;/strong&gt;).&lt;/p&gt;

&lt;h2 id=&quot;controlling-complexity-of-decision-trees&quot;&gt;Controlling complexity of decision trees&lt;/h2&gt;

&lt;p&gt;Typically, building a tree as described here and continuing until all leaves are terminal leads to models that are very complex and highly overfit to the training data. The presence of terminal leaves mean that a tree is 100% accurate on the training set; each data point in the training set is in a leaf that has the correct majority class.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/012.png&quot; alt=&quot;pair_plot&quot; width=&quot;70%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The overfitting can be seen in the image above. You can see the regions determined to
belong to class 1 in the middle of all the points belonging to class 0. On the other hand, there is a small strip predicted as class 0 around the point belonging to class 0 to the very right.  This is not how one would imagine the decision boundary to look, 
and the decision boundary focuses a lot on single outlier points that are far away
from the other points in that class.&lt;/p&gt;

&lt;p&gt;There are two strategies to prevent overfitting&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;stopping the creation of the tree early (also called pre-pruning)&lt;/li&gt;
  &lt;li&gt;building the tree but then removing or collapsing nodes that contain little information (also called pos-pruning)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Possible criteria for pre-pruning include limiting the maximum depth of the tree, limiting the maximum number of leaves, or requiring a minimum number of points in a node to keep splitting it.&lt;/p&gt;

&lt;p&gt;If we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep and complex. Unpruned trees are therefore prone to overfitting and not generalizing well to new data.&lt;/p&gt;

&lt;h2 id=&quot;regression-decision-trees-vs-linear&quot;&gt;Regression (Decision Trees vs Linear)&lt;/h2&gt;

&lt;p&gt;All that was said here on decision trees for classification is similarly true for decision trees for regression. The usage and analysis of regression trees is very similar to that classification trees. However, there is a particular property that is important to point out, though. All the tree based regression models are not able to &lt;em&gt;extrapolate&lt;/em&gt;, or make predictions outside of the range of the training data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/013.png&quot; alt=&quot;pair_plot&quot; width=&quot;70%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The difference between the models is quite striking. The linear model approximates
the data with a line, as we knew it would. This line provides quite a good forecast for
the test data (the years after 2000), while glossing over some of the finer variations in both the training and the test data. The tree model, on the other hand, makes perfect predictions on the training data; we did not restrict the complexity of the tree, so it learned the whole dataset by heart. However, once we leave the data range for which the model has data, the model simply keeps predicting the last known point. The tree has no ability to generate “new” responses, outside of what was seen in the training data. This shortcoming applies to all models based on trees.&lt;/p&gt;

&lt;p&gt;It is actually possible to make very good forecasts with tree-based models (for example, when trying to predict whether a price will go up or down). The point of this example was not to show that trees are a bad model for time series, but to illustrate a particular property of how trees make predictions.&lt;/p&gt;

&lt;h2 id=&quot;strenghts-weaknesses-and-parameters&quot;&gt;Strenghts, Weaknesses, and Parameters&lt;/h2&gt;

&lt;p&gt;Decision trees have two advantages over many of the algorithms we’ve discussed so far: the resulting model &lt;strong&gt;can easily be visualized and undersood by nonexperts&lt;/strong&gt; (at least for small trees), and &lt;strong&gt;the algorithms are completely invariant to scalling of the data&lt;/strong&gt;. As each feature is processed separately, and the possible splits of the data don’t depend on scaling, no preprocessing like normalization or standarization of features is needed for decision tree algorithms. In particular, decision trees work well when you have features that are completely different scales, or a mix of binary and continuous features.&lt;/p&gt;

&lt;p&gt;The main downside of decision trees is that even with the use of pre-pruning, they tend to overfit and provide poor generalization performance. Therefore, in most applications, the ensemble methods are usually used in place of a single decision tree.&lt;/p&gt;

&lt;h1 id=&quot;ensembles-of-decision-trees&quot;&gt;Ensembles of Decision Trees&lt;/h1&gt;

&lt;p&gt;Ensembles are methods that combine multiple machine learning models to create a more powerful models. There are many models in the machine learning literature that belong to this category, but there are two ensemble models that have proven to be effective on a wide range of datasets for classification and regression, both of which use decision trees as their atomic modules: &lt;strong&gt;random forests&lt;/strong&gt; and &lt;strong&gt;gradient boosted trees&lt;/strong&gt;.&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="Regression" /><category term="Classification" /><category term="Strenghts" /><category term="Weaknesses" /><summary type="html">Decision trees are widely used models for classification and regression tasks. Essentially, they predict a target by learning decision rules from features. As the name suggests, we can think of this model as breaking down our data by making a decision based on asking a series of questions.</summary></entry><entry><title type="html">Lasso vs Ridge Regression</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Lasso-vs-Ridge.html" rel="alternate" type="text/html" title="Lasso vs Ridge Regression" /><published>2021-05-11T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Lasso-vs-Ridge</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Lasso-vs-Ridge.html">&lt;p&gt;In practice, ridge regression is usually the first choice between these two models. However, if you have a large amount of features and expect only a few of them to be important, Lasso might be a better choice. Similarly, if you would like to have a model that is easy to interpret, Lasso will provide a model that is easier to under‐ stand, as it will select only a subset of the input features. scikit-learn also provides the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice, this combination works best, though at the price of having two parameters to adjust: one for the L1 regularization, and one for the L2 regularization.&lt;/p&gt;

&lt;h2 id=&quot;l1-regularization-vs-l2-regularization&quot;&gt;L1 Regularization vs L2 Regularization&lt;/h2&gt;

&lt;p&gt;In order to create a less complex model when you have a large number of features in a dataset are used Regularization techniques to address overfitting and feature selection.&lt;/p&gt;

&lt;p&gt;As was said before, the regression model that use L1 Regularization technique is Lasso Regression and model which uses L2 Regularization technique is Ridge Regression. The key difference between these two is the penalty term.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ridge Regression&lt;/strong&gt; adds “square magnitude” of coefficient as penalty term to the loss function. The second part of the formula bellow represents the L2 Regularization.&lt;/p&gt;

\[\sum^n_{i=1}(y_i - \sum^p_{j=1}x_{ij}\Theta_j)^2 + \lambda\sum^p_{j=1}\Theta^2_j\]

&lt;p&gt;If the \(\lambda\) elemet is 0, then we will get back the ordinary least square, whereas a very large value it will add to much wieght and it will lead to underfit. Hence, it’s important how &lt;em&gt;lambda&lt;/em&gt; is chosen. This technique works very well to avoid overfit issues.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lasso Regression&lt;/strong&gt; adds &lt;em&gt;“absolute value of magnitude”&lt;/em&gt; of coefficient as penalty term to the loss function.&lt;/p&gt;

\[\sum^n_{i=1}(y_i - \sum^p_{j=1}x_{ij}\Theta_j)^2 + \lambda\sum^p_{j=1}|\Theta_j|\]

&lt;p&gt;Again, lambda zero it will get back de ordinary least square whereas very large value will make coefficients zero hence it will underfit.&lt;/p&gt;

&lt;p&gt;The key different between this techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for &lt;strong&gt;feature selection&lt;/strong&gt; in case we have a huge number of features.&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="Linear Models" /><category term="Regression" /><summary type="html">In practice, ridge regression is usually the first choice between these two models. However, if you have a large amount of features and expect only a few of them to be important, Lasso might be a better choice. Similarly, if you would like to have a model that is easy to interpret, Lasso will provide a model that is easier to under‐ stand, as it will select only a subset of the input features. scikit-learn also provides the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice, this combination works best, though at the price of having two parameters to adjust: one for the L1 regularization, and one for the L2 regularization.</summary></entry><entry><title type="html">Naive Bayes Classifier</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Naive-Bayes-Classifiers.html" rel="alternate" type="text/html" title="Naive Bayes Classifier" /><published>2021-05-11T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Naive-Bayes-Classifiers</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Naive-Bayes-Classifiers.html">&lt;p&gt;Naive Bayes Classifiers are a family of classifiers &lt;strong&gt;very similar to the linear models. However, they tend to be even faster in training.&lt;/strong&gt; The price paid for this efficiency is that Naive Bayes models &lt;strong&gt;provide generalization performance slightly worse&lt;/strong&gt; than that of linear classifiers like &lt;em&gt;Logistic Regression&lt;/em&gt; and &lt;em&gt;Linear SVC&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The reason why Naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collect simples per-class statistics from each feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.&lt;/p&gt;

&lt;h3 id=&quot;bayestheorem&quot;&gt;Bayes’Theorem&lt;/h3&gt;

&lt;p&gt;Bayes’Theorem finds the probability of an event occurring given the probability of another event that has already occured.&lt;/p&gt;

\[P(A \mid B) = \frac{ P(B \mid A)P(A)}{P(B)}\]

&lt;ul&gt;
  &lt;li&gt;Basically, we are trying to find probability of event A, given the event B is true. Event B is also termed as &lt;strong&gt;evidence&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;\(P(A)\) is the &lt;strong&gt;priori&lt;/strong&gt; of A (i.e. the probability of event before evidence is seen). The evidence is an attribute value of an unknown instance.&lt;/li&gt;
  &lt;li&gt;\(P(A \mid B)\) is a posteriori probability of B, i.e the probabily of event after evidence is seen.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-naive-bayes-works&quot;&gt;How Naive Bayes Works?&lt;/h3&gt;

&lt;p&gt;I have a training data set of weather and corresponding target ‘Play’. Now we need to classify whetever players will play or not based on weather condition. Example from &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/&quot;&gt;Analytics Vidhya&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Step 1: Convert the data into a frequency table&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/009.png&quot; alt=&quot;pair_plot&quot; width=&quot;70%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;
&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 2: Create Likehood table by finding the probabilities&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/010.png&quot; alt=&quot;pair_plot&quot; width=&quot;70%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;
&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with highest probability is the outcome prediction.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Problem: Players will play if weather is sunny. This statement is correct?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;\(P(Yes \mid Sunny) = \frac{P(Sunny \mid Yes)P(YES)}{P(Sunny)}\)
\(P(Yes \mid Sunny) = \frac{\frac{3}{9} \cdot \frac{9}{14}}{\frac{5}{14}} = 0.60\text{, which is a higher probability}\)&lt;/p&gt;

&lt;h3 id=&quot;naive-bayes-classifiers-variants&quot;&gt;Naive Bayes Classifiers Variants&lt;/h3&gt;

&lt;p&gt;There are three kinds of Naive Bayes classifiers: &lt;strong&gt;GuassianNV&lt;/strong&gt;, &lt;strong&gt;BernoulliNB&lt;/strong&gt;, and &lt;strong&gt;MultinomialNV&lt;/strong&gt;. GuassianNB can be applied to any countinuous data, while BernoulliNV assumes binary data as MultinomailNB assumes count data (that is, each feature represents an integer count of something, like often a word appers in a sentence). BernoulliNB and MultinomialNB are mostly used in text data classification.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Guassian&lt;/strong&gt; it is used in classification and it assumes that features follow a normal distribution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multinominal&lt;/strong&gt; it is used for discrete counts. For example, let’s say, we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of as “number of times outcome number \(x_i\) is observed over the \(n\) trials”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Binomial&lt;/strong&gt; the binomial model is useful if your feature vectores are binary. One application would be text classification, where the 1s and 0s are “word occurs in the document” and “word does not occur in the document” respectively.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make prediction, a data point is compared to the statistics for each of the classes, and the best matching class is predicted. Interestingly, for both MultinomialNB and BernoulliNB, this leads to a prediction formula that is of the same form as in the linear models.&lt;/p&gt;

&lt;h2 id=&quot;strenghts-weaknesses-and-parameters&quot;&gt;Strenghts, Weaknesses, and Parameters&lt;/h2&gt;

&lt;p&gt;MultinomialNB and BernoulliNB have a single parameter, &lt;em&gt;alpha&lt;/em&gt;, which controls model complexity. The way alpha works is that the algorithm adds to the data alpha many virtual points that have positive values for all the features. This results in a “smoothing” of the statistics. A large alpha means more smoothing, resulting in less complex models. The algorithm’s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance. However, tuning it usually improves accuracy somewhat.&lt;/p&gt;

&lt;p&gt;GuassianNB is mostly used on very high-dimensional data, while the other two variants of naive Bayes are widely used for sparse count data such as text. MultinominalNB usually performs better than BinaryNB, particulary on datasets with a relatively large number of nonzero features (i.e. large documents).&lt;/p&gt;

&lt;p&gt;The Naive Bayes models share many of the strenghts and weaknesses of the linear models. &lt;strong&gt;They are very fast to train and to predict, and the training procedure is easy to understand.&lt;/strong&gt; The models work very well with high-dimensional sparse data and are relatively robust to the parameters. Naive Bayes models are great baseline models and are often used on very large datasets, where training even a liner model might take too long.&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="Linear Models" /><category term="Classification" /><category term="Strenghts" /><category term="Weaknesses" /><summary type="html">Naive Bayes Classifiers are a family of classifiers very similar to the linear models. However, they tend to be even faster in training. The price paid for this efficiency is that Naive Bayes models provide generalization performance slightly worse than that of linear classifiers like Logistic Regression and Linear SVC.</summary></entry><entry><title type="html">Random Forests</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Random-Forests.html" rel="alternate" type="text/html" title="Random Forests" /><published>2021-05-11T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Random-Forests</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/11/Random-Forests.html">&lt;p&gt;A main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. A random forest is essentially a collection of decision trees, where each tree is slightly different from the others. The idea behind this is that each tree might do a relatively good job of predicting, but will likely overfit in different ways, we can reduce the amount of overfitting by averaging their results.&lt;/p&gt;

&lt;h2 id=&quot;how-to-build-a-random-forest&quot;&gt;How to build a Random Forest?&lt;/h2&gt;

&lt;p&gt;First of all, you need to &lt;strong&gt;decide the number of trees to build&lt;/strong&gt; (the &lt;em&gt;n_estimators&lt;/em&gt; parameter). These trees will be built completely independently from each other, and the algorithm will make different random choices for each tree to make sure the trees are distinct. To build a tree, we first take what we called a &lt;em&gt;bootstrap sample&lt;/em&gt; of our data. That is, from out &lt;em&gt;n_samples&lt;/em&gt; data points, we repeatedly draw an example randomly with replacement, &lt;em&gt;n_sample&lt;/em&gt; times. This will create a dataset that is a big as the original dataset, but some data points will be missing from it, and some will be repeated.&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="Regression" /><category term="Classification" /><category term="Strenghts" /><category term="Weaknesses" /><summary type="html">A main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. A random forest is essentially a collection of decision trees, where each tree is slightly different from the others. The idea behind this is that each tree might do a relatively good job of predicting, but will likely overfit in different ways, we can reduce the amount of overfitting by averaging their results.</summary></entry><entry><title type="html">K-Nearest Neighbors</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/K-NN.html" rel="alternate" type="text/html" title="K-Nearest Neighbors" /><published>2021-05-10T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/K-NN</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/K-NN.html">&lt;p&gt;The &lt;em&gt;K&lt;/em&gt;-NN algorithm is the arguably the simplest machine learning algorithm. Building the model consists only of storing the training dataset. To make a prediction for a new data point, the algorithm finds the closest data points in the training set.&lt;/p&gt;

&lt;h2 id=&quot;k-neighbors-classification&quot;&gt;&lt;em&gt;K&lt;/em&gt;-Neighbors Classification&lt;/h2&gt;

&lt;p&gt;In its simple version, the &lt;em&gt;K&lt;/em&gt;-NN algorithm only considers exactly one nearest neighbor, which is the closest training data point to the point we want to make the predicition for. The point that we want to predict will take the label from the nearest neighbor (image 1). Instead of considering only the closest neighbor, we can also consider an arbitrary number, &lt;em&gt;K&lt;/em&gt;, of neighbors. When considering more than one neighbor, we use voting to assign a label (image 2).&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;figure class=&quot;image&quot;&gt;
  &lt;div style=&quot;display: flex; flex-direction: row;&quot;&gt;
    &lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/001.png&quot; alt=&quot;pair_plot&quot; width=&quot;50%&quot; /&gt;
    &lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/002.png&quot; alt=&quot;pair_plot&quot; width=&quot;50%&quot; /&gt;
  &lt;/div&gt;
    &lt;figcaption style=&quot;font-size: 16px&quot;&gt;Value of the target&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;k-neighbors-regression&quot;&gt;&lt;em&gt;K&lt;/em&gt;-Neighbors Regression&lt;/h2&gt;

&lt;p&gt;There is also a regression variant of &lt;em&gt;K&lt;/em&gt;-nearest neighbors algorithm. The prediction using a single neighbor is just the target value of the nearest neighbor (image 1). Again, we can use more than one neighbor for regression. When using multiple nearest neighbors, the prediction is the average of the relevant neighbors (image 2).&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;figure class=&quot;image&quot;&gt;
  &lt;div style=&quot;display: flex; flex-direction: row;&quot;&gt;
    &lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/003.png&quot; alt=&quot;pair_plot&quot; width=&quot;50%&quot; /&gt;
    &lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/004.png&quot; alt=&quot;pair_plot&quot; width=&quot;50%&quot; /&gt;
  &lt;/div&gt;
    &lt;figcaption style=&quot;font-size: 16px&quot;&gt;Value of the target&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;strenghts-weaknesses-and-parameters&quot;&gt;Strenghts, Weaknesses, and Parameters&lt;/h2&gt;

&lt;p&gt;There are two important parameters to the KNeighbors classifier: &lt;strong&gt;the number of neighbors&lt;/strong&gt; and &lt;strong&gt;how to measure distance between points&lt;/strong&gt;. In practice, using a small number of neighbors increases the complexibility of the model, and using many neighbors corresponds to a much simple model. Considering a single nearest neighbor, the prediction on the training set is perfect. But when more neighbors are considered, the model becomes simpler and the training accuracy drops. The test set accuracy for using a single neighbor is lower than when using more neighbors, indicating that using the single nearest neighbor leads to a model that is too complex. On the other hand, when considering to much neighbors, the model is too simple and performance is even worse. The best performance is somewhere in the middle. By default, Euclidean distance is used, which works well in many situations.&lt;/p&gt;

&lt;p&gt;One of the strenghts of &lt;em&gt;K&lt;/em&gt;-NN is that the model is &lt;strong&gt;very easy to understand&lt;/strong&gt;, and often &lt;strong&gt;gives reasonable performance without a lot of adjustments&lt;/strong&gt;. Using this algorithm is a good baseline to try before considering more advanced techniques.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Building the nearest neighbors model is usually very fast, but when your training set is very large (either in number of samples or number of features) prediction can be slow.&lt;/strong&gt; When using the &lt;em&gt;K&lt;/em&gt;-NN algorithm, it’s very important to preprocess your data. This approach does not perform well on datasets with many features, and it does particularly badly with data sets where most features are 0 most of the time (so called &lt;em&gt;sparse datasets&lt;/em&gt;).&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="KNN" /><category term="Regression" /><category term="Classification" /><category term="Strenghts" /><category term="Weaknesses" /><summary type="html">The K-NN algorithm is the arguably the simplest machine learning algorithm. Building the model consists only of storing the training dataset. To make a prediction for a new data point, the algorithm finds the closest data points in the training set.</summary></entry><entry><title type="html">Lasso Regression</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/Lasso-Regression.html" rel="alternate" type="text/html" title="Lasso Regression" /><published>2021-05-10T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/Lasso-Regression</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/Lasso-Regression.html">&lt;p&gt;An alternative to Ridge for regularizing linear regression is Lasso. As with ridge regression, using the lasso also restricts coefficients to be close to zero, but in a slightly different way, called L1 regularization. The consequence of L1 regularization is that when using the lasso, some coefficients are exactly zero. This means some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to interpret, and can reveal the most important features of your model.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lasso&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;lasso&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lasso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training set score: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Test set score: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Number of features used: {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Out:
Training set score: 0.29
Test set score: 0.21
Number of features used: 4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, Lasso does quite badly, both on the training and the test set. This indicates that we are underfitting, and we find that it used only 4 of the 105 features. Similarly to Ridge, the Lasso also has a regularization parameter, &lt;em&gt;alpha&lt;/em&gt;, that controls how strongly coefficients are pushed toward zero. In the previous example, we used the default of alpha=1.0. To reduce underfitting, let’s try decreasing alpha. When we do this, we also need to increase the default setting of max_iter (the maximum number of iterations to run).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# we increase the default setting of &quot;max_iter&quot;, 
# otherwise the model would warn us that we should increase max_iter.
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lasso001&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lasso&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training set score: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Test set score: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Number of features used: {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lasso001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Out:
Training set score: 0.90
Test set score: 0.77
Number of features used: 33
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A lower alpha allowed us to fit a more complex model, which worked better on the training and test data. The performance is slightly better than using Ridge, and we are using only 33 of the 105 features. This makes this model potentially easier to understand.&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="Linear Models" /><category term="Regression" /><summary type="html">An alternative to Ridge for regularizing linear regression is Lasso. As with ridge regression, using the lasso also restricts coefficients to be close to zero, but in a slightly different way, called L1 regularization. The consequence of L1 regularization is that when using the lasso, some coefficients are exactly zero. This means some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to interpret, and can reveal the most important features of your model.</summary></entry><entry><title type="html">Linear Models</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/LinearModels.html" rel="alternate" type="text/html" title="Linear Models" /><published>2021-05-10T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/LinearModels</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/LinearModels.html">&lt;p&gt;Linear models make a prediction using a &lt;em&gt;linear function&lt;/em&gt; of the input features.&lt;/p&gt;

&lt;h2 id=&quot;linear-models-for-regression&quot;&gt;Linear Models for Regression&lt;/h2&gt;

&lt;p&gt;For regression, the general prediction for a linear model looks as follows:&lt;/p&gt;

\[y = \Theta_0 + \Theta_1x_1 + \Theta_2x_2 + \Theta_px_p\]

&lt;p&gt;Here, \(x_0\) to \(x_p\) denotes the features (in this example, the number of features is p) of a single data point, \(\Theta\) are parameters of the model that are learned, and \(y\) is the prediction the model makes.&lt;/p&gt;

&lt;p&gt;Linear models for regression can be characterized as regression models for which the prediction is a line for a single feature, a plane when using two features, or a hyperplane in a higher dimensions.&lt;/p&gt;

&lt;p&gt;For datasets with many features, linear models can be very powerful. In particular, if you have more features thain training data points, any target &lt;em&gt;y&lt;/em&gt; can be perfectly modeled (on the training set) as a linear function.&lt;/p&gt;

&lt;p&gt;There are many different linear models for regression. The difference between these models lies in how the models parameters \(\Theta\) are learned from the training data, and how model complexity can be controlled.&lt;/p&gt;

&lt;h2 id=&quot;linear-models-for-classification&quot;&gt;Linear Models for Classification&lt;/h2&gt;

&lt;p&gt;The formula looks very similar to the one for linear regression, but instead of returning the weighted sum of features, we threshold the predicted value at zero. If the function is smaller than zero, we predict the class -1; if it is larger than zero, we predict the class +1. The prediction rule is common to linear models for classification. Again, there are many different ways to find the coefficients \(\Theta\) and the intercept \(\Theta_0\).&lt;/p&gt;

\[y = \Theta_0 + \Theta_1x_1 + \Theta_2x_2 + \Theta_px_p &amp;gt; 0\]

&lt;p&gt;For linear models for regression, the output, \(y\), is a linear function of the features: a line, plane, or hyperplane. For linear models for classification, the &lt;em&gt;decision boundary&lt;/em&gt; is a linear function of the input. In other words, a (binary) linear classifier is a classifier that separates two classes using a line, a plane or a hyperplane.&lt;/p&gt;

&lt;p&gt;The two most common linear classification algorithms are &lt;em&gt;Logistic Regression&lt;/em&gt; and &lt;em&gt;Linear Support Vector Machine&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The two models come up with similar decision boundaries. By default, both models apply an L2 regularization, in the same way that Ridge does for regression.&lt;/p&gt;

&lt;p&gt;For &lt;em&gt;Logistic Regression&lt;/em&gt; and &lt;em&gt;Linear SVM&lt;/em&gt; the trade-off parameter that determines the strength of the regularization is called &lt;em&gt;C&lt;/em&gt;, and higher values of &lt;em&gt;C&lt;/em&gt; correspond to less regularization. In other words, when you use a high value of &lt;em&gt;C&lt;/em&gt;, &lt;em&gt;Logistic Regression&lt;/em&gt; and &lt;em&gt;Linear SVM&lt;/em&gt; try to fit the training set as best as possible, while with low values of the parameter &lt;em&gt;C&lt;/em&gt;, the models put more emphasis on finding a coefficient vector \(\Theta\) that is close to zero.&lt;/p&gt;

&lt;p&gt;Another interessant aspect is how the parameter &lt;em&gt;C&lt;/em&gt; acts. Using low values of &lt;em&gt;C&lt;/em&gt; will cause the algorithms to try adjust to the “majority” of data points, while using a higher value of &lt;em&gt;C&lt;/em&gt; stresses the importance that each individual data point be classified correctly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/007.png&quot; alt=&quot;pair_plot&quot; width=&quot;90%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Similarly to the case of regression, linear models for classification might seem very restrictive in low-dimensional spaces, only allowing for decision boundaries that are straight lines or planes. Again, in high dimensions, linear models for classification become very powerful, and guarding against overfitting becomes increasingly important when considering more features.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_breast_cancer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_breast_cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stratify&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cancer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;logreg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training set score: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logreg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Test set score: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logreg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Out:
Training set score: 0.953
Test set score: 0.958
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The default value of \(C=1\) provides quite good performance, with \(95\%\) accuracy on both the training and the test set. But as training and test set performance are very close, it is likely that we are underfitting. Let’s try to increase &lt;em&gt;C&lt;/em&gt; to fit a more flexible model:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logreg100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training set score: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logreg100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Test set score: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logreg100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Out:
Training set score: 0.972
Test set score: 0.965
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using \(C=100\) results in higher training set accuracy, and also a slightly increased test set accuracy, confirming our intuition that a more complex model should perform better. We can also investigate what happens if we use an even more regularized model than the default of \(C=1\), by setting \(C=0.01\):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logreg001&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training set score: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logreg001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Test set score: {:.3f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logreg001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Out:
Training set score: 0.934
Test set score: 0.930
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As expected, when moving more to the left along the scale from an already underfit model, both training and test set accuracy decrease relative to the default parameters.&lt;/p&gt;

&lt;h2 id=&quot;linear-models-for-multiclass-classification&quot;&gt;Linear Models for Multiclass Classification&lt;/h2&gt;

&lt;p&gt;A common technique to extend a binary classification algorithm to a multiclass classification algorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is learned for each class that tries to separate that class from all of the other classes, resulting in as many binary models as there are classes. To make a prediction, all binary classifiers are run on a test point. The classifier that has the highest score on its single class “wins, ” and this class label is returned as the prediction. Let’s visualize the lines given by the three binary classifiers&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/008.png&quot; alt=&quot;pair_plot&quot; width=&quot;70%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;You can see that all the points belonging to class 0 in the training data are above the line corresponding to class 0, which means they are on the “class 0” side of this binary classifier. The points in class 0 are above the line corresponding to class 2, which means they are classified as “rest” by the binary classifier for class 2. The points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as “rest.” Therefore, any point in this area will be classified as class 0 by the final classifier (the result of the classifica‐ tion confidence formula for classifier 0 is greater than zero, while it is smaller than zero for the other two classes).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But what about the triangle in the middle of the plot?&lt;/strong&gt; All three binary classifiers classify points there as “rest”. Which class would a point there be assigned to? The answer is the one with the highest value for the classification formula: the class of the closest line.&lt;/p&gt;

&lt;h2 id=&quot;strenghts-weaknesses-and-parameters&quot;&gt;Strenghts, Weaknesses, and Parameters&lt;/h2&gt;

&lt;p&gt;The main parameter of linear models is the regularization parameter, called &lt;em&gt;alpha&lt;/em&gt; in the regression models and &lt;em&gt;C&lt;/em&gt; in &lt;em&gt;Linear SVM&lt;/em&gt; and &lt;em&gt;Logistic Regression&lt;/em&gt;. Large values for &lt;em&gt;alpha&lt;/em&gt; or small values for &lt;em&gt;C&lt;/em&gt; mean simple models. In particular for the regression models, tunning these parameters is quite important. Usually &lt;em&gt;C&lt;/em&gt; and &lt;em&gt;alpha&lt;/em&gt; are searched for on a logarithmic scale. The other decision importante to make is whetever you want to use L1 regularization or L2 regularization. If you assume that only a few of your features are actually important, you should use L1. Otherwise, you should default to L2. L1 is actually important if interpretability of the model is important. As L1 will use only a few features, it is easier to explain which features are important to the model, and what features of these features are.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Linear features are very fast to train, and also fast to predict. They scale to very large datasets and work well with sparse data.&lt;/strong&gt; If your data consists of hundreds of thousands or millions of samples, you might want to investigate using the \(solver = 'sag'\) option in &lt;em&gt;Logistic Regression&lt;/em&gt; and &lt;em&gt;Ridge&lt;/em&gt;, which can be faster than the default on large datasets.&lt;/p&gt;

&lt;p&gt;Other options are the &lt;em&gt;SGDClassifier&lt;/em&gt; class and the &lt;em&gt;SGDRegressor&lt;/em&gt; class, which implement even more scalable versions of the linear models described here.&lt;/p&gt;

&lt;p&gt;Another stregth of linear models is that they make it relatively easy to understand how a prediction is made, using the formulas we saw earlier for regression and classification. Unfortunally, it is often not entirely clear why coefficients are the way they are. This is particularly true if your dataset has highly correlated features; in these cases, the coefficients might be hard to interpret.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Linear models often perform well when the number of features is large compared to the number of samples.&lt;/strong&gt; &lt;strong&gt;They are also often used on very large datasets, simply because it’s not feasible to train other models.&lt;/strong&gt; However, in lower-dimensional spaces, other models might be yield better generalization performance.&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="Linear Models" /><category term="Multiclass" /><category term="Regression" /><category term="Classification" /><category term="Strenghts" /><category term="Weaknesses" /><summary type="html">Linear models make a prediction using a linear function of the input features.</summary></entry><entry><title type="html">Linear Regression (aka ordinary least squares)</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/LinearRegression.html" rel="alternate" type="text/html" title="Linear Regression (aka ordinary least squares)" /><published>2021-05-10T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/LinearRegression</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/LinearRegression.html">&lt;p&gt;Linear regression, or &lt;em&gt;ordinary least squares&lt;/em&gt; (OLS), is the simplest and most classic linear method for regression. Linear regression finds the parameters \(\Theta\) that minimize the &lt;em&gt;mean squared error&lt;/em&gt; between predictions and the true regression targets, &lt;em&gt;y&lt;/em&gt;, on the training set. &lt;strong&gt;The &lt;em&gt;mean squared error&lt;/em&gt; is the sum of the squared differences between the predictions and the true values.&lt;/strong&gt; Linear regression has no parameters which is a benefit, but is also has no way to control model complexity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/005.png&quot; alt=&quot;pair_plot&quot; width=&quot;70%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mglearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_wave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The values of \(\Theta_i , i=1, ..., N\), also called &lt;em&gt;weights&lt;/em&gt; or &lt;em&gt;coefficients&lt;/em&gt;, are stored in the &lt;em&gt;coef_&lt;/em&gt; atribbute, while the offset or &lt;em&gt;intercept (\(\Theta_0\)) is stored in the *intercept_&lt;/em&gt; attribute:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lr.coef_: {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lr.intercept_: {}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intercept_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Out:
lr.coef_: [ 0.394]
lr.intercept_: -0.031804343026759746
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s look at the training set and test set performance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training set score: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Test set score: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Out:
Training set score: 0.67
Test set score: 0.66
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;An \(R^2\) of around 0.66 is not very good, but we can see that the scores on the training
and test sets are very close together. This means we are likely underfitting, not overfitting.  For this one-dimensional dataset, there is little danger of overfitting, as the model is very simple. However, with higher-dimensional datasets, linear models become more powerful, and there is a higher chance of overfitting.&lt;/p&gt;

&lt;p&gt;If there was a huge discrepancy between performance on the training data and the test data is a clear sign of overfitting, and therefore we should try to find a model that allows us to control complexity.&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="Linear Models" /><category term="Regression" /><summary type="html">Linear regression, or ordinary least squares (OLS), is the simplest and most classic linear method for regression. Linear regression finds the parameters \(\Theta\) that minimize the mean squared error between predictions and the true regression targets, y, on the training set. The mean squared error is the sum of the squared differences between the predictions and the true values. Linear regression has no parameters which is a benefit, but is also has no way to control model complexity.</summary></entry><entry><title type="html">Ridge Regression</title><link href="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/Ridge-Regression.html" rel="alternate" type="text/html" title="Ridge Regression" /><published>2021-05-10T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/Ridge-Regression</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/supervised%20learning/2021/05/10/Ridge-Regression.html">&lt;p&gt;Ridge is also a linear model for regression, so the formula it uses to make predictions is the same one used for ordinary least squares. In ridge regression, thought, the coefficients \(\Theta\) are chosen not only so that they predict well on the training data, but also to fit an additional constraint. We also want the magnitude of coefficients to be as small as possible; in other words, all entries of \(\Theta\) should be close to 0. Intuitively, this means each feature should have as little effect on the outcome as possible (which translates to having a small slope), while still predicting well. This constraint is an example of what is called regularization. Regularization means explicitly restricting a model to avoid overfitting. The particular kind used by ridge regression is known as L2 regularization.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ridge&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ridge&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training set score: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Test set score: {:.2f}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ridge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Out:
Training set score: 0.89
Test set score: 0.75
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ridge is a more restricted model, so we are less likely to overfit. A less complex model means worse performance on the training set, but better generalization. The Ridge model makes a trade-off between the simplicity of the model (near-zero coefficients) and its performance on the training set. How much importance the model places on simplicity versus training set performance can be specified by the user, using the alpha parameter. In the previous example, we used the default parameter alpha=1.0. The optimum setting of alpha depends on the particular dataset we are using. Increasing alpha forces coefficients to move more toward zero, which decreases training set performance but might help generalization. A higher alpha means a more restricted model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000//machine-learning-algorithms-review/assets/images/006.png&quot; alt=&quot;pair_plot&quot; width=&quot;70%&quot; style=&quot;margin: auto; display: block; &quot; /&gt;&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Supervised Learning" /><category term="Supervised Learning" /><category term="Linear Models" /><category term="Regression" /><summary type="html">Ridge is also a linear model for regression, so the formula it uses to make predictions is the same one used for ordinary least squares. In ridge regression, thought, the coefficients \(\Theta\) are chosen not only so that they predict well on the training data, but also to fit an additional constraint. We also want the magnitude of coefficients to be as small as possible; in other words, all entries of \(\Theta\) should be close to 0. Intuitively, this means each feature should have as little effect on the outcome as possible (which translates to having a small slope), while still predicting well. This constraint is an example of what is called regularization. Regularization means explicitly restricting a model to avoid overfitting. The particular kind used by ridge regression is known as L2 regularization.</summary></entry><entry><title type="html">Machine Learning Fundamentals</title><link href="http://localhost:4000/machine-learning-algorithms-review/machine%20learning/2021/05/09/machine-learning-fundamentals.html" rel="alternate" type="text/html" title="Machine Learning Fundamentals" /><published>2021-05-09T00:00:00+01:00</published><updated>2021-05-12T21:22:53+01:00</updated><id>http://localhost:4000/machine-learning-algorithms-review/machine%20learning/2021/05/09/machine-learning-fundamentals</id><content type="html" xml:base="http://localhost:4000/machine-learning-algorithms-review/machine%20learning/2021/05/09/machine-learning-fundamentals.html">&lt;h2 id=&quot;problems-machine-learning-can-solve&quot;&gt;Problems Machine Learning Can Solve&lt;/h2&gt;

&lt;p&gt;The most successful machine learning algorithms are those that automate decision-making processes by generelizing from known examples. In this setting, which is known as &lt;em&gt;Supervised Learning&lt;/em&gt;, the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired output. &lt;em&gt;Unsupervised Learning&lt;/em&gt; are the other type of algorithm that only the input is known, and no known output data is given to the algorithm.&lt;/p&gt;

&lt;h2 id=&quot;measuring-sucess-training-and-testing-data&quot;&gt;Measuring Sucess: Training and Testing data&lt;/h2&gt;

&lt;p&gt;Before we can apply our model to new measurements, we need to know whether it actually works - that is, whether we should trust its predictions. First of all, we cannot use the data we used to build the model to evaluated it. This is because our model can always remember the whole training set, and will therefore always predict the correct label for any point in the training set. “Remembering” dont tell us whether our model &lt;strong&gt;generelize&lt;/strong&gt; well.&lt;/p&gt;

&lt;p&gt;To test the model’s performance, we show it new data for which we have labels. This is usually done by splitting the labeled data we collected into two parts. One part is used to build our machine learning model, called &lt;em&gt;training data&lt;/em&gt; and the other part will be used to test the performance of our model, called &lt;em&gt;test data&lt;/em&gt;.&lt;/p&gt;</content><author><name>Daniel Martins</name></author><category term="Machine Learning" /><category term="Supervised Learning" /><category term="Machine Learning" /><category term="problems" /><category term="datasets" /><summary type="html">Problems Machine Learning Can Solve</summary></entry></feed>